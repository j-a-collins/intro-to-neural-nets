{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLT: Introduction to Neural Networks\n",
    "## Tensors, Arrays, Vectors - a brief summary\n",
    "##### (And a quick recap of dot product and vector addition)\n",
    "###### J-A-Collins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are “Tensors?”\n",
    "**Tensors** are closely-related to arrays. If you interchange tensor/array/matrix when it comes \n",
    "to machine learning, generaly people probably won’t give you too much of a hard time. But there are subtle differences related to either the context or attributes of the tensor object.  \n",
    "\n",
    "To understand a tensor, let’s compare and describe some of the other data containers in Python - that is, things that hold data. Let’s start with a list. A Python list is defined by comma-separated objects \n",
    "contained in square brackets. So far, we’ve been using lists in our previous notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example of a list:\n",
    "a_list = [1, 5, 6, 2]\n",
    "\n",
    "# Example of a list of lists:\n",
    "list_of_lists = [[1, 5, 6, 2],\n",
    "                 [3, 2, 1, 3]]\n",
    "# And an example of a list of lists of lists!\n",
    "\n",
    "list_of_list_lists = [[[1, 5, 6, 2], \n",
    "                       [3, 2, 1, 3]], \n",
    "                      [[5, 2, 1, 2], \n",
    "                       [6, 4, 8, 4]], \n",
    "                      [[2, 8, 5, 3], \n",
    "                       [1, 1, 9, 4]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything shown so far could also be an array or an array representation of a tensor. A list is just \n",
    "a list, and it can do pretty much whatever it wants, including:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_list_of_lists = [[4, 2, 3], \n",
    "                         [5, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this last list of lists in the previous cell cannot be an array because it is not **homologous**. Rememeber, a list of lists is homologous if each list along a dimension is identically long, and this must be true for each dimension. In the case of the list shown above, it’s a 2-D list. The first dimension’s length is the number of sublists in the total list (2). The second dimension is the length of each of those sublists (3, 2). In the above example, when reading across the “row” dimension (also called the second dimension), we can see that the first list is 3 elements long, and the second list is 2 elements long — this is not homologous and, therefore, cannot be an array. While failing to be consistent in one dimension is enough to show that this example is not homologous, we could also read down the “column” dimension (the first dimension); the first two columns are 2 elements long while the third column only contains 1 element. Note that every dimension does not necessarily need to be the same length; it is perfectly acceptable to have an array with 4 rows and 3 columns (i.e., 4x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrices:\n",
    "A matrix is simple. It’s a rectangular array. It has columns and rows and it's 2-D - so a matrix can be an array (a 2-D array to be precise).  \n",
    "\n",
    "Can all arrays be matrices? No. An array can be far more than just columns and rows, as it could have four dimensions, twenty dimensions, and so on. Let's look further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_matrix_array = [[4, 2], \n",
    "                     [5, 1], \n",
    "                     [8, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list in the previous cell could also be a valid matrix (because of its columns and rows), which automatically means it could also be an array. The “shape” of this array would be 3x2, or (3, 2), as it has 3 rows and 2 columns.  \n",
    "\n",
    "To denote a shape, we need to check every dimension. As we’ve already learned, a matrix is a \n",
    "2-D array. The first dimension is what’s inside the most outer brackets, and if we look \n",
    "at the above matrix, we can see 3 lists there: [4, 2], [5, 1], and [8, 2]; thus, the size in this \n",
    "dimension is 3 and each of those lists has to be the same shape to form an array (and matrix in this \n",
    "case). The next dimension’s size is the number of elements inside this more inner pair of brackets, \n",
    "and we see that it’s 2 as all of them contain 2 elements.  \n",
    "\n",
    "With 3-D arrays, like in list_of_list_lists below, we’ll have a third level of brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_list_lists = [[[1, 5, 6, 2], \n",
    "                       [3, 2, 1, 3]], \n",
    "                      [[5, 2, 1, 2], \n",
    "                       [6, 4, 8, 4]], \n",
    "                      [[2, 8, 5, 3], \n",
    "                       [1, 1, 9, 4]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first level of this array contains 3 matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix 1\n",
    "[[1, 5, 6, 2],\n",
    " [3, 2, 1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix 2\n",
    "[[5, 2, 1, 2],\n",
    " [6, 4, 8, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix 3\n",
    "[[2, 8, 5, 3],\n",
    " [1, 1, 9, 4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s what’s inside the most outer brackets - so the size of this dimension is 3. If we look \n",
    "at the first matrix, we can see that it contains 2 lists — [1, 5, 6, 2] and [3, 2, 1, 3] so the \n",
    "size of this dimension is 2 — while each list of this inner matrix includes 4 elements. These 4 \n",
    "elements make up the third and last dimension of this matrix since there are no more inner brackets. \n",
    "Therefore, the shape of this array is (3, 2, 4) and it’s a 3-D array, since the shape contains 3 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors:\n",
    "As far as I can tell, when it comes to the discussion of tensors versus arrays in the context \n",
    "of computer science, pages and pages of debate have ensued. The debate appears to be caused by the fact that people are arguing from entirely different places. There’s no question that a tensor is not just an array, but the real question is: _“What is a tensor, to a computer scientist, in the \n",
    "context of deep learning?”_. I think this can be resolved reasonably simply though:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### A tensor object is an object that can be represented as an array\n",
    "\n",
    "What this means is, as programmers, data scientists and engineers, we can (and will) treat tensors as arrays in the context of deep learning, and that’s really all the thought we have to put into it.  \n",
    "\n",
    "Are all tensors just arrays? No, but they are represented as arrays in our code, so, to us, they’re only arrays, and this is why there’s so much argument and confusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these notebooks, I define an array as an **ordered homologous container** for numbers, and mostly use this term when working with NumPy since that’s what the main data type is called within it. A linear array, also called a 1-D array, is the simplest example of an array, and in plain Python, this would be called a list. Arrays can also consist of multi-dimensional data, and one of the best-known examples is what we call a matrix in mathematics, which we’ll represent as a 2-D array. Each element of the array can be accessed using a tuple of indices as a key, which means that we can retrieve any array element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectors:\n",
    "Which brings us finally to one last definiton — the vector. Put simply, a vector in maths is what we call a list in Python or a 1-D array in NumPy. Of course, lists and NumPy arrays do not have the same properties as a vector, but, just as we can write a matrix as a list of lists in Python, we can also write a vector as a list or an array! Additionally, we’ll look at the vector algebraically (mathematically) as a set of numbers in brackets. This is in contrast to the physics perspective where, from my bakground, a vector's representation is usually seen as an arrow, characterized by having both a magnitude and a direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus material:\n",
    "#### Dot Product and Vector Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector multiplication, is one of the most important operations we’ll perform on vectors. We can achieve the same result as in our pure Python implementation of multiplying each element in our inputs and weights vectors element-wise by using a dot product, which I’ll explain shortly for those that need the revision. We can certainly refer to what we’re doing here as working with vectors just as we can call them “tensors.” Nevertheless, this all seems to add to the mysticism of neural networks — mysterious objects out in a complex multi-dimensional vector space that we’ll never quite understand. But it's fine, let's Keep thinking of vectors as arrays — a 1-D array is just a vector (or a list in \n",
    "Python).  \n",
    "\n",
    "Because of the sheer number of variables and interconnections made, we can model very complex and non-linear relationships with non-linear activation functions, and truly feel like wizards, but this might do more harm than good. Yes, we will be using the “dot product,” but we’re doing this because it results in a clean way to perform the necessary calculations. It’s nothing more in-depth than that — as you’ve already seen, we can do this math with far more rudimentary-sounding terminology and approaches. So when multiplying vectors, we either perform a dot product or a cross product. A cross product results in a vector while a dot product results in a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by writing out the dot product of two vectors more formally:\n",
    "\n",
    "![dot-product-of-two-vectors.JPG](img/dot-product-of-two-vectors.JPG \"The dot product of two vectors\") \n",
    "\n",
    "So we can see that a dot product of two vectors is a sum of products of consecutive vector elements. Both vectors must be of the same size (have an equal number of elements). Let's code it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare two vectors:\n",
    "a = [1, 2, 3]\n",
    "b = [2, 3, 4]\n",
    "\n",
    "# Calculate the dot product:\n",
    "dot_product = a[0] * b[0] + a[1] * b[1] + a[2] * b[2]\n",
    "dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see, the previous cell is the code-based version of the following:  \n",
    "\n",
    "![dot-product-calc.JPG](img/dot-product-calc.JPG \"The dot product calculated\")  \n",
    "\n",
    "Now, imagine if we called _a_ “inputs” and _b_ “weights?” This dot product them looks like a succinct way to perform the operations we need and have already performed in Python. We need to multiply our weights and inputs at the same index values and then add the resulting values together. The dot product performs this exact type of operation; thus, it makes lots of sense to use it here. Returning to the neural network code, let’s make use of this dot product in the next notebook using NumPy. We’ll also need to perform a **vector addition** operation at some point. Fortunately, NumPy lets us perform this in a natural way — using the plus sign with the variables containing vectors of the data. The addition of the two vectors is an operation performed element-wise, which means that both vectors have to be of the same size, and the result will become a vector of this size also. The result is a vector calculated as a sum of the consecutive vector elements:\n",
    "\n",
    "![vector-addition.JPG](img/vector-addition.JPG \"Vector Addition\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
