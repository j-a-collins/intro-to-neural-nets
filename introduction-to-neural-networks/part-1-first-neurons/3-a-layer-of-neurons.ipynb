{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLT: Introduction to Neural Networks\n",
    "## A Layer of Neurons\n",
    "##### J-A-Collins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks typically have layers that consist of more than one single neuron. Layers are simply groups of neurons. Each neuron in a layer takes exactly the same input — the input \n",
    "given to the layer (which can be either the training data or the output from the previous layer), \n",
    "but contains its own set of weights and its own bias, producing its own unique output. The layer’s \n",
    "output is a set of each of these outputs — one per neuron.  \n",
    "\n",
    "Let’s say we have a scenario with 3 neurons in a layer and 4 inputs. We’ll keep the same 4 inputs and set of weights for the first neuron the same as we’ve been using so far. Then we’ll add 2 additional, made up, sets of weights and 2 additional biases to form 2 new neurons for a total of 3 in the layer. The layer’s output is going to be a list of 3 values, not just a single value like for a single neuron. Let's build it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs, weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "weights1 = [0.2, 0.8, -0.5, 1]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then calculating the outputs - a list of 3 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [\n",
    " # For Neuron 1:\n",
    " inputs[0]*weights1[0] +\n",
    " inputs[1]*weights1[1] +\n",
    " inputs[2]*weights1[2] +\n",
    " inputs[3]*weights1[3] + bias1,\n",
    " # For Neuron 2:\n",
    " inputs[0]*weights2[0] +\n",
    " inputs[1]*weights2[1] +\n",
    " inputs[2]*weights2[2] +\n",
    " inputs[3]*weights2[3] + bias2,\n",
    " # For Neuron 3:\n",
    " inputs[0]*weights3[0] +\n",
    " inputs[1]*weights3[1] +\n",
    " inputs[2]*weights3[2] +\n",
    " inputs[3]*weights3[3] + bias3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's visualise the network:\n",
    "\n",
    "![layered-neuron.JPG](img/layered-neuron.JPG \"Simple Layered Neuron Schematic\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have constructed three sets of weights and three biases, which define three neurons. Each \n",
    "neuron is _connected_ to the same inputs. The difference is in the separate weights and bias \n",
    "that each neuron applies to the input. This is called a **fully connected** neural network — every \n",
    "neuron in the current layer has connections to every neuron from the previous layer.  \n",
    "\n",
    "This is a common form of neural network, but it should be noted that there is no requirement to \n",
    "fully connect everything like this. At this point, we have only shown code for a single layer \n",
    "with very few neurons. Imagine coding many more layers and more neurons. This would get \n",
    "very challenging to code using our current methods. Instead, we could use a loop to scale and \n",
    "handle dynamically-sized inputs and layers. Instead, let's turn the separate weight variables into a \n",
    "list of weights so we can iterate over them, and then change the code to use loops instead of hardcoded operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inputs remain the same, but now we re-arrange the weights and biases into more helpful forms:\n",
    "weights = [weights1, weights2, weights3]\n",
    "biases = [bias1, bias2, bias3]\n",
    "# Let's also make an empty list for the current layer outputs:\n",
    "layer_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each neuron:\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    # Zeroed output of given neuron\n",
    "     neuron_output = 0\n",
    "     # For each input and weight to the neuron\n",
    "     for n_input, weight in zip(inputs, neuron_weights):\n",
    "         # Multiply this input by associated weight and add to the neuron's output variable:\n",
    "         neuron_output += n_input * weight\n",
    "     # Add the bias\n",
    "     neuron_output += neuron_bias\n",
    "     # Append neuron's result to the layer's output list:\n",
    "     layer_outputs.append(neuron_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look:\n",
    "layer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does the same thing as before, but in more dynamic and scalable way.  \n",
    "\n",
    "Some help for those in the team that might not be super strong with Python just yet:  \n",
    "**zip()** lets us iterate over multiple iterables (lists in this case) simultaneously. The first item in each passed iterator is paired together, and then the second item in each passed iterator are paired together etc.\n",
    "\n",
    "That’s it for our first layer of neurons! How do we know we have three neurons? Why do we have three? We can tell we have three neurons because there are 3 sets of weights and 3 biases. When you make a neural network of your own, you also get to decide how many neurons you want for each of the layers. You can combine however many inputs you are given with however many neurons that you would like. As far as I can tell, the more that I've built neural nets, I've slowly gained some intuition as to how many neurons to try using for a given problem. In these notebooks we will start by using trivial numbers of neurons to aid in understanding how neural networks work at their core.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
