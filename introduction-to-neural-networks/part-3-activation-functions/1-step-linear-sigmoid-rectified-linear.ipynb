{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLT: Introduction to Neural Networks\n",
    "## Step, Linear, Sigmoid, and Rectified Linear Activation Functions\n",
    "###### J-A-Collins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at some of the activation functions and discuss their roles. We use different activation functions for different situations, understanding how they work can us choose which of them is appropriate for a given task. The activation function is applied to the output of a neuron (or a layer of neurons), which modifies outputs. We use activation functions because if the activation function itself is **nonlinear**, it allows for neural networks with usually two or more hidden layers to map nonlinear functions.  \n",
    "\n",
    "\n",
    "In general, neural network's will have two types of activation functions. The first will be the \n",
    "activation function used in the hidden layers, and the second function will be used in the output layer. Usually, the activation function used for hidden neurons will be the same for all of them, but this isn't always necessarily the case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Activation Function\n",
    "This activation function serves to mimic a neuron [“firing” or “not firing”](https://www.verywellmind.com/what-is-an-action-potential-2794811) based on input information. The simplest version of this is a **step function**. In a single neuron, if the weights·inputs + bias results in a value > 0, the neuron will fire and output a 1; otherwise, it will output a 0.\n",
    "\n",
    "![step.png](img/step.png \"Step Function Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Activation Function\n",
    "This one is simply the equation of a line. It will appear as a straight line when graphed, where _y=x_ and the output value equals the input. This activation function is usually applied to the last layer’s output in the case of a regression model (a model that outputs a scalar value instead of a classification)\n",
    "\n",
    "![linear.png](img/linear.png \"Linear Function Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Sigmoid Activation Function\n",
    "The problem with the step function is it’s not really very informative. When we get to training and network optimisers, we'll note that the way an optimiser works is by assessing individual impacts that weights and biases have on a network’s output. The problem with a step function is that it’s less clear to the optimiser what these impacts are because there’s very little information gathered from this function. It’s either 1 or 0, on or off. It’s hard to tell how “close” this kind of step function was to activating or deactivating. Maybe it was close, or maybe it was far. In terms of the final output value from the network, it doesn’t matter if it was close to outputting something else. Therefore, when it comes time to optimise weights and biases, it’s easier for the optimiser if we have activation functions that are more granular and informative. A more granular function is the **Sigmoid activation** function:\n",
    "\n",
    "![sigmoid.png](img/sigmoid.png \"Sigmoid Function Graph\")\n",
    "\n",
    "\n",
    "This function returns a value in the range of 0 for negative infinity, through 0.5 for the input of 0, \n",
    "and to 1 for positive infinity. As I mentioned in a previous notebook, with “dead neurons,” it’s usually better to have a more granular approach for the hidden neuron activation functions. In this case, we’re getting a value that can be reversed to its original value; the returned value contains all the information from the input, unlike a function like the step function, where an input of 3 will output the same value as an input of 300,000. The output from the Sigmoid function, being in the range of 0 to 1, also works better with neural networks — especially compared to the range of the negative to the positive infinity — and _adds nonlinearity_. The Sigmoid function, as far as I can tell, was historically used in hidden layers, but after a time was replaced by the **Rectified Linear Units activation function** (or _ReLU_). That said, I will still use the Sigmoid function as the output layer’s activation function in some later examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Rectified Linear Activation Function\n",
    "The rectified linear activation function is simpler than the sigmoid. It’s quite literally y=x, clipped \n",
    "at 0 from the negative side. If x is less than or equal to 0, then y is 0 — otherwise, y is equal to x. This simple yet powerful activation function is the most widely used activation function as far as I can tell — this is mainly due to speed and efficiency. While the sigmoid activation function isn’t the most complicated, it’s still much more challenging to compute than the _ReLU_ activation function. \n",
    "\n",
    "![relu.png](img/relu.png \"ReLU Function Graph\")\n",
    "\n",
    "The _ReLU_ activation function is extremely close to being a linear activation function while remaining nonlinear, due to the bend after 0. This simple property is, however, very effective"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
