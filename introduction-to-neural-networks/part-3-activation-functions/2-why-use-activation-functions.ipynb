{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLT: Introduction to Neural Networks\n",
    "## Why use Activation Functions?\n",
    "###### J-A-Collins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from the previous notebook what activation functions represent, how some of them look, and what \n",
    "exactly they return, but we haven't really addressed _why_ we would use activation functions in the first place. In the great many cases, for a neural network to fit a **nonlinear function**, we need it to contain two or more hidden layers, and we need those hidden layers to use a nonlinear  activation function. To start, let's ask the question: _what is a nonlinear function_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's think about a _sine function_. It **cannot** be represented well by a straight line:\n",
    " \n",
    " ![sine-func.png](img/sine-func.png \"Sine Function Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to think of problems that are linear in nature: trying to figure out the cost of some number of books. If we say that we know the cost of an individual book, and that there are no discounts, then the equation to calculate the price of any number of books is a **linear equation**. \n",
    "\n",
    "As data scientists at Aviva though, we're more interested in problems that are not so simple. An example of a nonlinear function could be the price of a home. The number of factors that come into play, such as size, location, time of year attempting to sell, number of rooms, yard, neighborhood, and so on, makes the pricing of a home a nonlinear equation. Many of the more interesting and hard problems of our time are nonlinear. The main attraction for neural networks has to do with their ability to solve nonlinear problems. First, letâ€™s consider a situation where neurons have no activation function, which would be the same as having an activation function of _y=x_. With this linear activation function in a neural network with 2 hidden layers of 8 neurons each, the result of training this model will look like:\n",
    "\n",
    "![lin-func-fit-sine.png](img/lin-func-fit-sine.png \"A Neural net with linear activation functions attempting to fit y=sin(x)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the same 2 hidden layers of 8 neurons each with the **rectified linear activation** function, we see the following result after training:\n",
    "\n",
    "![relu-sine.png](img/relu-sine.png \"ReLU activation functions fitting y=sin(x)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
