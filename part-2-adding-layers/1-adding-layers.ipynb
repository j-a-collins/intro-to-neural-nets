{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLT: Introduction to Neural Networks\n",
    "## Adding Layers, Training Data, Dense Layer Class\n",
    "###### J-A-Collins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Layers:\n",
    "So far, the neural network we’ve built is quite simple as we have only one layer. Neural networks are considered “deep” when they consist of 2 or more _hidden layers_. Currently, our single layer is effectively an output layer. Why we want two or more hidden layers will be something we explore in later notebooks.  \n",
    "\n",
    "A hidden layer isn’t an input or output layer; as the data scientist, you see data as they are coming in to the input layer and then again as the resulting data from the output layer. Layers between these endpoints have values that we don’t necessarily deal with direcetly, hence the name “_hidden_”. However, don’t let this name trick you into thinking that we can’t access these values. We will often use them to diagnose issues or improve the neural network. To better explore this concept, let’s start by adding another layer to the neural network, and, for now, let’s assume these two layers that we’re going to have will be the hidden layers, and the output layer just isn't programmed yet. Before we add another layer, let’s think about what will be coming. In the case of the first layer, we can see that we have an input with 4 features.\n",
    "\n",
    "![input_layer_4_features.png](img/input_layer_4_features.png \"Input layer with 4 features into a hidden layer with 3 neurons\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Samples** (feature set data) get fed through the input, which does not change it in any way, to our \n",
    "first hidden layer, which we can see has 3 sets of weights, with 4 values each. Each of those 3 unique weight sets is associated with its distinct neuron. Thus, since we have 3 weight sets, we have 3 neurons in this first hidden layer. Each neuron has a unique set of weights, of which we have 4 (as there are 4 inputs to this layer), which is why our initial weights have a shape of (3, 4). Now, we want to add another layer. To do this, we need to make sure that the expected input to that layer matches the previous layer’s output. We have set the number of neurons in a layer by setting how many weight sets and biases we have. The previous layer’s influence on weight sets for the current layer is that each weight set needs to have a separate weight per input. This means a distinct weight per neuron from the previous layer (or feature if we’re talking the input). The previous layer has 3 weight sets and 3 biases, so we know it has 3 neurons. This then means, for the next layer, we can have as many weight sets as we want (because this is how many neurons this new layer will have), but each of those weight sets must have 3 discrete weights. To create this new layer, we can just copy and paste our weights and biases to _weights2_ and _biases2_, and change their values to new made up sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = [[1, 2, 3, 2.5],\n",
    "          [2., 5., -1., 2],\n",
    "          [-1.5, 2.7, 3.3, -0.8]]\n",
    "\n",
    "weights = [[0.2, 0.8, -0.5, 1],\n",
    "           [0.5, -0.91, 0.26, -0.5],\n",
    "           [-0.26, -0.27, 0.17, 0.87]]\n",
    "\n",
    "biases = [2, 3, 0.5]\n",
    "\n",
    "weights2 = [[0.1, -0.14, 0.5],\n",
    "            [-0.5, 0.12, -0.33],\n",
    "            [-0.44, 0.73, -0.13]]\n",
    "\n",
    "biases2 = [-1, 2, -0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, inputs to layers are either inputs from the dataset you’re training with \n",
    "# or outputs from a previous layer. This is why, in the above cell we defined 2 versions of \n",
    "# weights and biases but only 1 of inputs — because the inputs for layer 2 will be \n",
    "# the outputs from the previous layer\n",
    "\n",
    "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
    "layer2_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data:\n",
    "Rather than continue using random values as our data, we can use a helper function to from an external package to create some non-linear data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Put package somewhere:\n",
    "!pip install \"C:\\Users\\jacka\\Jupyter Notebooks\\introduction-to-neural-networks\\nnhelper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnhelper.datasets import spiral_data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, we won't be generating training data from a function for the neural networks we make - instead we will have an actual dataset. Using this package is just a way to ensure repeatability for users of the notebooks.  \n",
    "\n",
    "The _nnhelper.init()_ does a few things: it sets the random seed to 0 (by default), creates a  float32 dtype default, and overrides the original dot product from _NumPy_. All of these are meant to ensure repeatable results for users that are following along. The spiral_data function allows us to create a dataset with as many classes as we want. The function has parameters for choosing the number of classes and the number of points per class in the resulting non-linear dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = spiral_data(samples=100, classes=3)\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, cmap='brg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the neural network will not be aware of the colour differences as the data have no class encodings. This is only made for instructive purposes to users of the notebooks. In the figure above, each point is the feature, and its coordinates are the samples that form the dataset. The “classification” for that point has to do with which spiral it is a part of, depicted by the various colours in the image. These colours would then be assigned a class number for the model to fit to (e.g, 0, 1, and 2).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer Class:\n",
    "Now that we no longer need to hand-type our data, we should create something similar for our various types of neural network layers. So far, we’ve only used what’s called a _dense_ or _fully-connected layer_ (fc). Our dense layer class will begin with two methods. Here's a starting outline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    " # Layer init:\n",
    " def __init__(self, n_inputs, n_neurons):\n",
    "     # Initialise weights and biases\n",
    "     pass # pass statement as a placeholder for now\n",
    " # Forward pass\n",
    " def forward(self, inputs):\n",
    "     # Calculate output values from inputs, weights, and biases\n",
    "     pass # pass statement as a placeholder for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, weights are often initialised *randomly* for a model, but not always. If we want to load a pre-trained model, we need to initialise the parameters to whatever that pre-trained model finished with. It’s also possible that, even for a new model, you have some other initialisation rules besides random. For now, we’ll stick with random initialisation. Next, we have the _forward_ method. When we pass data through a model from beginning to end, this is called a **forward pass**. Just like everything else, however, this is not the only way to do things. We can have the data loop back around and do other interesting things. We’ll keep it common and perform a regular forward pass.  \n",
    "\n",
    "\n",
    "We can amend the _Layer_Dense_ class’ code now by adding the random initialisation of weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "class Layer_Dense:\n",
    " # Layer init:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "     # Forward pass\n",
    "    def forward(self, inputs):\n",
    "     # Calculate output values from inputs, weights, and biases\n",
    "     pass # pass statement as a placeholder for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are setting weights randomly and the biases are set to 0. Note that we’re initialising \n",
    "weights to be _(inputs, neurons)_, rather than _(neurons, inputs)_. We’re doing this instead of \n",
    "transposing every time we perform a forward pass, as explained in the previous notebook.  \n",
    "\n",
    "Why are the biases zeros? In specific scenarios, like with many samples containing values of 0, a bias can ensure that a neuron fires initially. It sometimes may be appropriate to initialise the biases to some non-zero value, but the most common initialisation for biases is 0. However, in these scenarios, we may find success in doing things another way. This will vary depending on the use-case and is just \n",
    "one of many things that can be tweaked when trying to improve results. One situation where we might want to try something else is with what’s called **dead neurons**. I haven’t yet covered **activation \n",
    "functions** in practice, but we can start by imagining our step function:\n",
    "\n",
    "![activation.png](img/activation.png \" Graph of a step function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s possible for _weights · inputs + biases_ not to meet the threshold of the step function, which \n",
    "means the neuron will output a 0. Alone, this is not a big issue, but it becomes a problem if this \n",
    "happens to this neuron for every one of the input samples (it’ll become clear why once we cover \n",
    "**backpropagation**). So then this neuron’s 0 output is the input to another neuron. Any weight multiplied by zero will be zero obviously. With an increasing number of neurons outputting 0, more inputs to the next neurons will receive these 0s rendering the network essentially non-trainable, or “_dead_.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnhelper.init()\n",
    "print(np.random.randn(2, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s also explore _np.random.randn_ and _np.zeros_ in more detail. These methods are convenient ways to initialise arrays. np.random.randn produces a **Gaussian distribution** with a mean of 0 and a variance of 1, which means that it’ll generate (pseudo-)random numbers, positive and negative, centered at 0 and with the mean value close to 0. In general, neural networks work best with values between -1 and +1, which I’ll discuss later. So this _np.random.randn_ generates values around those numbers. We’re going to multiply the Gaussian distribution for the weights by 0.01 to generate numbers that are a couple of magnitudes smaller. Otherwise, the model will take more time to fit the data during the training process as starting values will be disproportionately large compared to the updates being made during training. The central idea here is to start a model with non-zero values small enough that they won’t affect training. This way, we have a bunch of values to begin working with, but hopefully none too large or as zeros. You can experiment with values other than 0.01 if you like. Finally, the _np.random.randn_ function takes dimension sizes as parameters and creates the output array with this shape. The weights here will be the number of inputs for the first dimension and the number of neurons for the 2nd dimension. This is similar to our previous 'made up' array of weights, just randomly generated.  \n",
    "\n",
    "So to see an example of how our method initialises weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 2\n",
    "n_neurons = 4\n",
    "weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "biases = np.zeros((1, n_neurons))\n",
    "print(weights)\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can update our forward method with the dot product+biases calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, inputs):\n",
    "    self.output = np.dot(inputs, self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make use of this new class instead of using hardcoded calculations. We can generate some data using the dataset creation method and use our new layer to perform a forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "x, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(x)\n",
    "# Let's see output of the first few samples:\n",
    "dense1.output[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The complete code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import nnhelper\n",
    "from nnhelper.datasets import spiral_data\n",
    "nnhelper.init()\n",
    "\n",
    "\n",
    "# Dense layer\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "\n",
    "x, y = spiral_data(samples=100, classes=3)\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "dense1.forward(x)\n",
    "print(dense1.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a network of neurons, so our neural network model is almost deserving of its name, but we’re still missing the **activation functions**, so we will work on those in the next notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
